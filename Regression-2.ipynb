{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbd4ffd4-03f2-4389-a323-9997d41b97e6",
   "metadata": {},
   "source": [
    "# 1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737b0d1c-e989-44d6-b91e-5034a0e3a20d",
   "metadata": {},
   "source": [
    "R-squared, also known as the coefficient of determination, is a statistical measure used in the context of linear regression models. It represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n",
    "\n",
    "\n",
    "R-squared quantifies how well the independent variables explain the variation in the dependent variable. An R-squared value ranges from 0 to 1:\n",
    "\n",
    "0 indicates that the model explains none of the variability of the response data around its mean.\n",
    "1 indicates that the model explains all the variability of the response data around its mean.\n",
    "Calculation:\n",
    "R-squared is calculated as follows:\n",
    "\n",
    "ùëÖ^2=1‚àí(SSres/SStot)\n",
    "\n",
    "\n",
    "Where:\n",
    "SSres(Residual Sum of Squares) is the sum of the squared differences between the observed values and the predicted values by the model.\n",
    "SStot(Total Sum of Squares) is the sum of the squared differences between the observed values and the mean of the observed values.\n",
    "\n",
    "SSres=‚àëi=1ton(yi‚àíy^i)^2\n",
    "\n",
    "SSres=‚àëi=1ton(yùëñ‚àíyÀâ)^2\n",
    "\n",
    "Where:\n",
    "\n",
    "yi is the actual value of the dependent variable.\n",
    "\n",
    "y^i is the predicted value of the dependent variable from the regression model.\n",
    "\n",
    "ùë¶Àâis the mean of the actual values.\n",
    "\n",
    "n is the number of observations.\n",
    "# Note:\n",
    "High R-squared: Indicates that a large proportion of the variance in the dependent variable is explained by the independent variables, suggesting a good fit of the model to the data.\n",
    "Low R-squared: Indicates that the independent variables do not explain much of the variance in the dependent variable, suggesting a poor fit of the model to the data.\n",
    "\n",
    "# Limitations:\n",
    "Overfitting: A high R-squared does not necessarily mean the model is good. It might be due to overfitting, especially if there are many independent variables.\n",
    "Comparisons: R-squared should not be used alone to compare models with different dependent variables or different datasets. Adjusted R-squared, which adjusts for the number of predictors in the model, can be more useful in such cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e5cbad-c7f2-457e-ba7f-dedd2e65d30a",
   "metadata": {},
   "source": [
    "## Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d59a4ff-53ee-45eb-a520-4fae83ef1221",
   "metadata": {},
   "source": [
    "Adjusted R-squared is a modified version of the regular R-squared that adjusts for the number of predictors in a model. Unlike the regular R-squared, which can only increase or remain the same when additional predictors are added to the model, adjusted R-squared can decrease if the added predictors do not improve the model sufficiently.\n",
    "\n",
    "Definition:\n",
    "Adjusted R-squared is calculated using the formula:\n",
    "Adjusted R-squared = 1-(1‚àíR^2)(n‚àí1)/n-p-1)\n",
    "\n",
    "R^2 is the regular R-squared.\n",
    "n is the number of observations.\n",
    "p is the number of predictors (independent variables) in the model.\n",
    "\n",
    "How it Differs from Regular R-squared:\n",
    "Penalizes for More Predictors: Adjusted R-squared incorporates a penalty for the number of predictors in the model, preventing overestimation of the model's explanatory power when unnecessary predictors are included. This makes it more reliable for comparing models with different numbers of predictors.\n",
    "\n",
    "Can Decrease: Unlike R-squared, which can only increase or stay the same when more predictors are added, adjusted R-squared can decrease if the additional predictors do not significantly improve the model's fit.\n",
    "\n",
    "Comparison Across Models: Adjusted R-squared is more suitable for comparing the goodness-of-fit of different models, especially when these models have different numbers of predictors. It gives a more accurate measure of how well the predictors explain the variance in the dependent variable while accounting for the number of predictors used.\n",
    "\n",
    "Interpretation:\n",
    "Higher Adjusted R-squared: Indicates that the model explains a larger proportion of the variance in the dependent variable, accounting for the number of predictors. A higher value suggests a better fit.\n",
    "Lower Adjusted R-squared: Indicates that the model does not explain much of the variance, or that adding more predictors does not improve the model sufficiently to justify their inclusion.\n",
    "Example:\n",
    "Consider two models:\n",
    "\n",
    "Model A: Uses 3 predictors and has an R^2 of 0.85.\n",
    "\n",
    "Model B: Uses 5 predictors and has an R^2 of 0.87.\n",
    "\n",
    "While Model B has a higher R^2, the adjusted ùëÖ^2 may be lower than Model A's if the additional predictors do not contribute significantly to explaining the variance in the dependent variable.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba368a9c-c85c-4986-8c62-fc2f4ea81529",
   "metadata": {},
   "source": [
    "## Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352fcfba-9857-49d1-b7ff-d3d5d125e7dd",
   "metadata": {},
   "source": [
    "Adjusted R-squared is more appropriate to use in the following scenarios:\n",
    "\n",
    "1. Comparing Models with Different Numbers of Predictors:\n",
    "When you have multiple regression models with different numbers of predictors, adjusted R-squared provides a better comparison. It accounts for the number of predictors, preventing the misleading increase in R-squared that can occur simply by adding more variables.\n",
    "\n",
    "2. Preventing Overfitting:\n",
    "If you are concerned about overfitting, adjusted R-squared helps by penalizing the addition of predictors that do not significantly improve the model. This ensures that only predictors that contribute meaningfully to explaining the variance are considered beneficial.\n",
    "\n",
    "3. Model Selection:\n",
    "When selecting the best model from a set of candidates, especially in stepwise regression or when using automated model selection techniques, adjusted R-squared helps in choosing a model that balances goodness-of-fit with complexity.\n",
    "\n",
    "4. Evaluating Model Performance:\n",
    "In scenarios where you are evaluating the performance of a model on a given dataset, adjusted R-squared provides a more realistic measure of how well the model explains the variability in the dependent variable, accounting for the number of predictors.\n",
    "\n",
    "5. Large Datasets with Many Predictors:\n",
    "In large datasets with many potential predictors, using adjusted R-squared helps in identifying the most parsimonious model that explains the data well without including too many unnecessary predictors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22d1e0a-8279-4ff7-93d9-4ab414166960",
   "metadata": {},
   "source": [
    "## Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca3c61f-0bf3-4abe-927d-4a06400b9323",
   "metadata": {},
   "source": [
    "In regression analysis, RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are metrics used to evaluate the performance of a regression model by quantifying the difference between predicted values and observed values.\n",
    "\n",
    "# MSE:\n",
    "\n",
    "MSE provides a measure of the average squared difference between predicted and actual values.\n",
    "A lower MSE indicates a better fit of the model to the data.\n",
    "Because it squares the errors, it penalizes larger errors more heavily than smaller ones.\n",
    "\n",
    "# RMSE: \n",
    "\n",
    "RMSE gives an estimate of the standard deviation of the prediction errors.\n",
    "Like MSE, a lower RMSE indicates a better fit of the model to the data.\n",
    "RMSE is more interpretable in the context of the dependent variable's units.\n",
    "\n",
    "# MAE:\n",
    "\n",
    "MAE provides a measure of the average absolute difference between predicted and actual values.\n",
    "A lower MAE indicates a better fit of the model to the data.\n",
    "Unlike MSE and RMSE, MAE does not square the errors, so it does not penalize larger errors more than smaller ones.\n",
    "\n",
    "# \n",
    "MSE quantifies the average squared errors and penalizes larger errors more heavily.\n",
    "\n",
    "RMSE is the square root of MSE and provides error metrics in the same units as the dependent variable, making it more interpretable.\n",
    "\n",
    "MAE quantifies the average absolute errors and treats all errors equally, regardless of their magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b786488e-ca6a-41a0-b668-39ecec759d88",
   "metadata": {},
   "source": [
    "# Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182eb66b-acbc-4ee3-9bc9-294896a4bee9",
   "metadata": {},
   "source": [
    "# Advantages and Disadvantages of Using RMSE, MSE, and MAE\n",
    "\n",
    "1. Mean Squared Error (MSE):\n",
    "\n",
    "Advantages:\n",
    "Mathematically Convenient: MSE is differentiable, making it useful for mathematical optimization techniques, such as gradient descent.\n",
    "Penalizes Larger Errors: By squaring the errors, MSE penalizes larger errors more than smaller ones, which can be beneficial when large errors are particularly undesirable.\n",
    "\n",
    "Disadvantages:\n",
    "Sensitive to Outliers: Since MSE squares the errors, it is highly sensitive to outliers, which can disproportionately affect the metric.\n",
    "Not Intuitive: The units of MSE are the square of the units of the dependent variable, which can make it less interpretable compared to other metrics.\n",
    "\n",
    "2. Root Mean Squared Error (RMSE):\n",
    "\n",
    "Advantages:\n",
    "Same Units as Dependent Variable: RMSE is in the same units as the dependent variable, making it more interpretable and easier to relate to the actual data.\n",
    "Sensitive to Larger Errors: Like MSE, RMSE penalizes larger errors more heavily, which is useful in scenarios where large errors are more significant.\n",
    "\n",
    "Disadvantages:\n",
    "Sensitive to Outliers: RMSE inherits the sensitivity to outliers from MSE due to the squaring of errors.\n",
    "Not Scale-Invariant: RMSE can be affected by the scale of the dependent variable, making it difficult to compare across different datasets without normalization.\n",
    "\n",
    "3. Mean Absolute Error (MAE):\n",
    "\n",
    "Advantages:\n",
    "Intuitive and Easy to Understand: MAE represents the average absolute error, making it straightforward to interpret.\n",
    "Robust to Outliers: MAE is less sensitive to outliers compared to MSE and RMSE since it does not square the errors.\n",
    "Scale-Invariant: MAE is less affected by the scale of the dependent variable compared to RMSE, making it more comparable across different datasets.\n",
    "\n",
    "Disadvantages:\n",
    "Less Sensitive to Larger Errors: MAE treats all errors equally, which might not be desirable in cases where larger errors should be penalized more.\n",
    "Non-Differentiable at Zero: MAE is not differentiable at zero, which can complicate optimization techniques that rely on gradient-based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda6a458-f55b-48ed-b0ef-728ded7fdcdc",
   "metadata": {},
   "source": [
    "# Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ca943-6ec5-46e1-9011-ec1918214d5b",
   "metadata": {},
   "source": [
    "Regularized linear models help prevent overfitting by adding a penalty term to the cost function used to fit the model. This penalty discourages the model from fitting the noise in the training data by constraining the coefficients, leading to simpler models that generalize better to new data.\n",
    "\n",
    "Ridge Regression (L2 Regularization):\n",
    "Cost Function:\n",
    "\n",
    "Penalty Term: \n",
    "ùúÜ‚àëùëó=1toùëù ùúÉùëó^2Œª\n",
    "\n",
    "Œª is the regularization parameter that controls the strength of the penalty.\n",
    "\n",
    "The penalty is the sum of the squares of the coefficients.\n",
    "\n",
    "Effect: Shrinks the coefficients towards zero but does not set them exactly to zero, reducing the model complexity and preventing overfitting by avoiding large coefficients that can fit the noise in the training data.\n",
    "\n",
    "# Lasso Regression (L1 Regularization):\n",
    "Penalty Term:\n",
    "\n",
    "ùúÜ‚àëùëó=1toùëù |ùúÉùëó|\n",
    "\n",
    "Œª is the regularization parameter that controls the strength of the penalty.\n",
    "The penalty is the sum of the absolute values of the coefficients.\n",
    "Effect: Can shrink some coefficients to exactly zero, effectively performing feature selection by removing irrelevant features, leading to a simpler model that is less likely to overfit.\n",
    "\n",
    "How Regularization Helps Prevent Overfitting:\n",
    "\n",
    "Constraining Coefficients: By adding a penalty for larger coefficients, regularization prevents the model from assigning too much importance to any single feature or set of features, thus reducing the likelihood of overfitting.\n",
    "\n",
    "Bias-Variance Trade-off: Regularization introduces bias into the model (by shrinking coefficients), but this is compensated by a reduction in variance. The result is a model that performs better on unseen data by not fitting the noise in the training data.\n",
    "\n",
    "Simpler Models: Regularization encourages simpler models with fewer and smaller coefficients, which tend to generalize better to new data.\n",
    "\n",
    "Feature Selection (Lasso): Lasso regularization can effectively reduce the number of features by setting some coefficients to zero, removing irrelevant or redundant features that could contribute to overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bc7c72-be06-45e6-834b-f1e09b101a3a",
   "metadata": {},
   "source": [
    "# Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17402d5c-439b-44d2-bfde-0b65f336f80a",
   "metadata": {},
   "source": [
    "Regularized linear models help to prevent overfitting in machine learning by adding a penalty term to the loss function. This penalty discourages the model from fitting the noise in the training data by constraining the size of the coefficients, which leads to simpler models that generalize better to new, unseen data.\n",
    "\n",
    "Example Scenario:\n",
    "Dataset:\n",
    "dataset with features \n",
    "\n",
    "X (predictors) and a target variable \n",
    "\n",
    "y (response). The goal is to build a regression model to predict \n",
    "\n",
    "y based on X.\n",
    "\n",
    "Overfitting Problem:\n",
    "Suppose we have a small dataset with many features. A standard linear regression model might overfit the training data, capturing not only the underlying relationship but also the noise.\n",
    "Overfitting results in a model with high variance, which performs well on the training data but poorly on test data.\n",
    "Ridge Regression (L2 Regularization):\n",
    "\n",
    "The term ùúÜ‚àëùëó=1toùëù ùúÉùëó^2Œª is the regularization term.\n",
    "\n",
    "Œª is the regularization parameter that controls the strength of the penalty. A larger \n",
    "\n",
    "Œª increases the penalty, leading to smaller coefficients.\n",
    "Effect:\n",
    "The regularization term discourages the model from assigning large weights to any feature, reducing the risk of overfitting.\n",
    "By shrinking the coefficients, the model becomes less sensitive to the noise in the training data.\n",
    "\n",
    "# Example Illustration:\n",
    "Without Regularization:\n",
    "Imagine fitting a linear regression model to a small dataset with 100 features and only 50 samples.\n",
    "\n",
    "The model might overfit, resulting in very high accuracy on the training data but poor performance on test data.\n",
    "\n",
    "# With Ridge Regression:\n",
    "Training Phase:\n",
    "\n",
    "The model is trained using Ridge Regression with Œª=1.0.\n",
    "The cost function includes the regularization term, which penalizes large coefficients.\n",
    "As a result, the model's coefficients are constrained, preventing them from becoming too large.\n",
    "Model Coefficients:\n",
    "\n",
    "Without regularization, some coefficients might be very large, indicating overfitting.\n",
    "With Ridge Regression, the coefficients are smaller, indicating a simpler model that is less likely to overfit.\n",
    "Performance:\n",
    "\n",
    "Training Data: The Ridge Regression model might have slightly lower accuracy compared to the non-regularized model because of the penalty.\n",
    "Test Data: The Ridge Regression model is likely to perform better on the test data because it generalizes better, having not fitted the noise in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a238d758-c6b4-4d09-98fd-8f28f972decf",
   "metadata": {},
   "source": [
    "# Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96513ff8-aa1c-4ea7-9a93-cd25f16e6a8d",
   "metadata": {},
   "source": [
    "Regularized linear models, while useful in preventing overfitting and improving generalization, have some limitations that may make them less suitable in certain contexts. Here are some key limitations and reasons why they may not always be the best choice for regression analysis:\n",
    "\n",
    "1. Assumption of Linearity:\n",
    "\n",
    "Limitation: Regularized linear models assume a linear relationship between the predictors and the response variable.\n",
    "\n",
    "Impact: If the true relationship is non-linear, these models may not capture the underlying patterns well, leading to poor performance.\n",
    "\n",
    "2. Choice of Regularization Parameter (Œª):\n",
    "\n",
    "Limitation: The performance of regularized linear models heavily depends on the choice of the regularization parameter.\n",
    "\n",
    "Impact: Determining the optimal value of \n",
    "\n",
    "Œª often requires cross-validation, which can be computationally expensive and time-consuming.\n",
    "\n",
    "3. Interpretability:\n",
    "\n",
    "Limitation: Regularized models, especially those with a large number of features, can be difficult to interpret.\n",
    "\n",
    "Impact: While Ridge Regression keeps all features, it can be hard to explain the impact of individual features when many coefficients are shrunk. \n",
    "\n",
    "Lasso can set some coefficients to zero, but the remaining non-zero coefficients can still be challenging to interpret in the context of feature interactions.\n",
    "\n",
    "4. Feature Scaling:\n",
    "\n",
    "Limitation: Regularized linear models are sensitive to the scale of the input features.\n",
    "\n",
    "Impact: Features need to be standardized or normalized before applying regularization. Without proper scaling, the penalty might disproportionately \n",
    "\n",
    "affect features with larger scales, leading to biased results.\n",
    "\n",
    "5. Handling of Irrelevant Features:\n",
    "\n",
    "Limitation: While Lasso can set some coefficients to zero, it may struggle with highly correlated features.\n",
    "\n",
    "Impact: Lasso may randomly select one feature from a group of correlated features and discard others, which might not always be desirable. Ridge tends to shrink coefficients of correlated features together, but does not eliminate irrelevant features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78d50e7-3b99-49bb-94e8-9e2a6a8e3a4c",
   "metadata": {},
   "source": [
    "# Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c6e555-dc21-41f6-a43e-5bede6400fe0",
   "metadata": {},
   "source": [
    "Choosing the better performer between Model A (with an RMSE of 10) and Model B (with an MAE of 8) requires understanding the strengths and limitations of RMSE and MAE, as well as the context of the problem.\n",
    "\n",
    "Understanding RMSE and MAE:\n",
    "# Root Mean Squared Error (RMSE):\n",
    "\n",
    "Definition: RMSE measures the square root of the average squared differences between predicted and actual values.\n",
    "\n",
    "Sensitivity: RMSE is more sensitive to outliers due to the squaring of errors, penalizing larger errors more heavily.\n",
    "\n",
    "Units: RMSE is in the same units as the dependent variable, making it interpretable.\n",
    "\n",
    "# Mean Absolute Error (MAE):\n",
    "\n",
    "Definition: MAE measures the average absolute differences between predicted and actual values.\n",
    "\n",
    "Sensitivity: MAE treats all errors equally, providing a linear score that is less sensitive to outliers.\n",
    "\n",
    "Units: MAE is also in the same units as the dependent variable, making it interpretable.\n",
    "Comparison of Models:\n",
    "\n",
    "# Model A: RMSE of 10\n",
    "\n",
    "Advantages: RMSE penalizes larger errors more heavily, which means Model A may perform better in scenarios where larger errors are particularly undesirable.\n",
    "\n",
    "Interpretation: The average squared deviation from the actual values is such that the square root of the mean is 10, indicating some presence of larger errors.\n",
    "\n",
    "# Model B: MAE of 8\n",
    "\n",
    "Advantages: MAE provides a linear measure of error, giving a straightforward interpretation of the average magnitude of errors.\n",
    "\n",
    "Interpretation: The average absolute deviation from the actual values is 8, suggesting a consistent performance across all errors without \n",
    "overemphasizing larger ones.\n",
    "\n",
    "Which Model to Choose?\n",
    "\n",
    "Context-Dependent Decision:\n",
    "If Larger Errors are Critical: If the problem context makes larger errors particularly undesirable (e.g., predicting doses of medication, financial forecasts with large losses), Model A with RMSE may be preferred despite having a higher MAE.\n",
    "\n",
    "If Consistency is Important: If the goal is to minimize the average error and treat all errors equally, Model B with MAE may be preferred.\n",
    "\n",
    "Limitations of the Choice:\n",
    "\n",
    "portance of large errors vs. consistent performance.\n",
    "\n",
    "Consider additional metrics if needed, such as R-squared, to understand the proportion of variance explained by the models.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d9ea17-d6fe-4310-a5a0-ea6df51d3619",
   "metadata": {},
   "source": [
    "# Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bc3f03-8fbf-4b31-86d9-08b8e55874ff",
   "metadata": {},
   "source": [
    "Choosing between Model A (Ridge regularization with Œª=0.1) and Model B (Lasso regularization with Œª=0.5) requires understanding the differences between Ridge and Lasso regularization, as well as their respective strengths and limitations.\n",
    "\n",
    "# Ridge Regularization (L2 Regularization):\n",
    "\n",
    "Effect: Shrinks coefficients towards zero but does not set them exactly to zero.\n",
    "\n",
    "Use Case: Works well when you have many small/medium-sized features, and multicollinearity is present. It is useful when all features are expected to contribute to the prediction.\n",
    "\n",
    "# Lasso Regularization (L1 Regularization):\n",
    "\n",
    "Effect: Can shrink some coefficients to exactly zero, effectively performing feature selection by removing irrelevant features.\n",
    "Use Case: Useful when you suspect that only a subset of the features is relevant, and you want a simpler model with fewer predictors.\n",
    "\n",
    "\n",
    "Comparison of Models:\n",
    "\n",
    "#Model A: Ridge Regularization (Œª=0.1)\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Stabilizes coefficients by shrinking them.\n",
    "Useful for handling multicollinearity.\n",
    "Retains all features, albeit with reduced coefficients.\n",
    "\n",
    "Disadvantages:\n",
    "Does not perform feature selection.\n",
    "May include irrelevant features with small coefficients.\n",
    "\n",
    "# Model B: Lasso Regularization (Œª=0.5)\n",
    "\n",
    "Advantages:\n",
    "Can produce sparse models by setting some coefficients to zero.\n",
    "Performs feature selection, which can lead to more interpretable models.\n",
    "\n",
    "Disadvantages:\n",
    "May arbitrarily select one feature among highly correlated features, disregarding others.\n",
    "High regularization parameter (Œª=0.5) might lead to excessive shrinking, removing relevant features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
